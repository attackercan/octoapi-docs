# 2 - Технический обзор ScanFactory   
----

## Вступление

На этой странице подробно описаны используемые инструменты и возможные параметры запуска. 

## Этапы сканирования

Платформа проводит сканирование внешнего периметра в 3 логических этапа:

 № | Описание этапа                | Результат работы  
--- | --- | ---  
 1 | Сбор активов (OSINT)          | Перечень доменов и IP, принадлежащих Заказчику  
 2 | Сканирование сетевых сервисов | Определение открытых портов и сервисов; поиск известных CVE; брутфорс аутентификации на сетевых сервисах (FTP, SSH, Telnet, и др)  
 3 | Сканирование веб-приложений   | Краулинг веб-сайтов; поиск скрытых файлов и папок; поиск уязвимостей  

Для примера рассмотрим сканирование Компании со следующими входными данными:  
Домены, принадлежащие компании: `*.company.ru`  
IP, принадлежащие компании: `11.22.33.44/23`  

<details>
<summary><h3>Этап 1. Сбор активов (OSINT)</h3></summary>

Доступен следующий набор инструментов:  

![amass](https://img.shields.io/badge/amass-blue.svg) и ![subfinder](https://img.shields.io/badge/subfinder-blue.svg) - Пассивный поиск поддоменов по открытым источникам
![altdns](https://img.shields.io/badge/altdns-blue.svg) - Рекурсивный брутфорс поддоменов по словарю (словарь по умолчанию - 1000 слов)
![sdto](https://img.shields.io/badge/sdto-blue.svg) - Проверка перехватов поддомена. Opensource инструмент собственной разработки

Пример работы:

- ![amass](https://img.shields.io/badge/amass-blue.svg) нашёл домен 2 домена: `www.company.ru` (3 уровня) и домен `sub.dev.company.ru` (4 уровня).  
  Оба домена добавлены в БД, и отображаются на вкладке Активы
- ![altdns](https://img.shields.io/badge/altdns-blue.svg) запустит брутфорс поддоменов на 3 уровень `*.company.ru` (т.к. в настройках был указан wildcard 3 уровня),  
  а также `*.dev.company.ru` (т.к. домен 4 уровня был найден с помощью ![amass](https://img.shields.io/badge/amass-blue.svg)).  
  Для этого создаётся 2 списка потенциальных доменов, и осуществляется резолв доменов с помощью инструмента ![massdns](https://img.shields.io/badge/massdns-blue.svg),  
  **публичных резолверов Yandex и Google**, со скоростью 1000 запросов в секунду.  
- На каждый найденный поддомен запускается проверка перехвата поддоменов с помощью инструмента ![sdto](https://img.shields.io/badge/sdto-blue.svg)

</details>

<details>

<summary><h3>Этап 2. Сканирование сетевых сервисов</h3></summary>

Выбранные шаблоны инструментов будут запущены на все IP из диапазона `11.22.33.44/23`, а также на все IP, в которые резолвятся обнаруженные домены из Этапа 1.  

Доступен следующий набор инструментов:  
![nmap](https://img.shields.io/badge/nmap-blue.svg) - поиск открытых портов, а также определение версий сервисов  
![infrascan](https://img.shields.io/badge/infrascan-blue.svg) - поиск известных CVE. База данных CVE является самой большой в мире, содержит более 65000 проверок, и обновляется каждый день  
![patator](https://img.shields.io/badge/patator-blue.svg) - брутфорс паролей к сетевым сервисам по дефолтным словарям  

При старте проекта создаётся 2 шаблона ![nmap](https://img.shields.io/badge/nmap-blue.svg): на топ-1000 портов; и на все 65535 портов. Также вы можете создать свой шаблон с нужными опциями ![nmap](https://img.shields.io/badge/nmap-blue.svg)  
На каждый открытый порт определяется версия запущенного сервиса, и запускается поиск известных уязвимостей к этой версии сервиса с помощью сервиса ![Vulners](https://img.shields.io/badge/Vulners-blue.svg)  

> Так как и все наши заказчики, и мы находимся в России - а значит, ответы о том, закрыт порт или нет, мы будем получать быстро, то мы используем разумные настройки, которые выполняют скан даже быстрее, чем `nmap timing option -Т4`, и при этом выдают достоверный результат

Мы используем следующие настройки для nmap:

`--max-rtt-timeout 600ms` - максимальное время получения ответа от порта  
`--min-rtt-timeout 10ms` - минимальное время получения ответа от порта  
`--initial-rtt-timeout 300ms` - первоначальное время получения ответа (нмап сам меняет это значение, в рамках min/max опций выше)  
`--host-timeout 110m` - максимальное время сканирования IP = 1 час 50 минут  
`--max-retries 3` - количество повторных попыток на сканирование порта  
`--max-scan-delay 100ms` - максимальная задержка между отправкой повторных пакетов  

С этими настройками 1000 портов на 1 сервере сканируются за 25 секунд. Таким образом, подсеть размера /23 (512 IP) на топ 1000 портов мы можем просканировать за 4,5 часа (имея 1 запущенную копию nmap)

**Пример работы**  
![nmap](https://img.shields.io/badge/nmap-blue.svg) обнаруживает открытый порт `22`, и опредляет версию запущенного сервиса как `OpenSSH 7.9`. Сервер помечается как "активный", и информацию по этому сервису теперь можно увидеть на вкладке Активы  
Сервис ![Vulners](https://img.shields.io/badge/Vulners-blue.svg) сообщает, что версия OpenSSH устарела и имеет перечень уязвимостей, поэтому создаёт новую запись об этой Уязвимости  
![infrascan](https://img.shields.io/badge/infrascan-blue.svg) запускает анализ каждого IP  
![patator](https://img.shields.io/badge/patator-blue.svg) запускает брутфорс авторизаций на каждый открытый порт  

**Полезный совет**  
При сканировании больших подсетей (более, чем /23) рекомендуем включать дополнительную настройку «Предварительный ping IP». В этом случае на каждый сервер отправляется команда `nmap -sn IP`, и на основе результата сервер будет отмечен как «Активный» или «Не активный». Далее, шаблоны инструментов будут запущены только на IP, определённые как «Активные».
</details>

<details>

<summary><h3>Этап 3. Сканирование веб-приложений</h3></summary>

Сканирование веб-приложений проходит в 3 этапа: сбор информации о веб-приложении, отсев «неинтересных» запросов, и поиск уязвимостей  

<details>
<summary><h4>Этап 3.1. Сбор информации о веб-приложении</h4></summary>

Выбранные шаблоны инструментов будут запущены **на все открытые порты**:

![crawler](https://img.shields.io/badge/crawler-blue.svg) - обход веб-приложения с помощью headless Chrome краулера собственной разработки, а также сохранение скриншотов главной страницы  
![crawlergo](https://img.shields.io/badge/crawlergo-blue.svg) - обход страниц веб-приложений  
![dirsearch](https://img.shields.io/badge/dirsearch-blue.svg) - поиск скрытых файлов и папок на веб-сервере  
Результат работы – набор GET/POST запросов, которые передаются на следующий этап
</details>

<details>
<summary><h4>Этап 3.2. Отсев «неинтересных» запросов</h4></summary>

Все обнаруженные запросы проходят через двухуровневую систему отсева «неинтересных» запросов:  
**Уровень 1. Отсев ссылок с похожим форматом**  
Пример. Во время краулинга интернет-магазина были найдены следующие ссылки:  
```text
http://client.ru/products/book-1/
http://client.ru/products/book-2/
http://client.ru/products/book-3/
...
http://client.ru/products/book-31337/
```
Все имеющиеся ссылки имеют одинаковый паттерн:  `http://client.ru/products/book-[\d]/`  

Как только в базу данных будет добавлено 50 (значение по умолчанию) ссылок с одинаковым паттерном, далее ссылки с таким же паттерном сохраняться не будут. В нашем примере будут сохранены только запросы к книгам с 1 по 50.  

**Уровень 2. Отсев страниц с похожим содержимым**  
ScanFactory использует собственную технологию, основанную на [SimHash](https://en.wikipedia.org/wiki/SimHash), которая анализирует контент каждой веб-страницы перед сохранением в базу данных.  
В случае, если большое количество страниц имеет одинаковую HTML-структуру (например, страницы блога, или карточки товаров), такие страницы не сохраняются  
</details>


<details>
<summary><h4>Этап 3.3. Поиск уязвимостей</h4></summary>

На финальном этапе запускаются следующие шаблоны инструментов:

![webscan](https://img.shields.io/badge/webscan-blue.svg) - сканер OWASP Top 10 уязвимостей. Инструмент работает в режиме proxy  
![nuclei](https://img.shields.io/badge/nuclei-blue.svg) - opensource сканер веб-приложений с набором публичных и приватных шаблонов.  
![ffuf](https://img.shields.io/badge/ffuf-blue.svg) - поиск скрытых GET/POST параметров путём поиска аномалий в ответах сервера. На каждый имеющийся HTTP-запрос в базе данных инструмент отправляет более 1000 запросов (вида `http://client.ru/api/?DEBUG=1` , `http://client.ru/api/ADMIN?=1` , и другие). Мы рекомендуем включать этот инструмент, только если у вас в скоупе 1-5 веб-приложений, и НЕ включать его, если веб-приложений больше  

**Сканирование веб-приложений методом «серого ящика»**  
Авторизация в веб-приложениях работает путём добавления авторизационных HTTP-заголовок к краулерам  
Для этого создайте новый шаблоны ![crawler](https://img.shields.io/badge/crawler-blue.svg) и ![crawlergo](https://img.shields.io/badge/crawlergo-blue.svg) с параметрами:  
`"extra_headers": {"Cookie": "a=b"}` (подставить сюда хэдеры, по которому приложение авторизует пользователя)  

Так как указанные хэдеры будут добавляться ко всем запросам краулера, то нужно создавать отдельный проект под каждое веб-приложение, для которого настраивается авторизация
</details>
</details>
